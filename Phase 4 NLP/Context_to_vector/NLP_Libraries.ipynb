{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishabh9559/Data_science/blob/main/Phase%204%20NLP/Context_to_vector/NLP_Libraries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK - Natural Language Toolkit"
      ],
      "metadata": {
        "id": "L56Fw9RzAkod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an open-source platform, NLTK provides a suite of programs and libraries that help developers, researchers, and students perform various Natural Language Processing (NLP) tasks.\n",
        "\n",
        "**It is used for things such as:**\n",
        "* Classification\n",
        "* Tokenization: Breaking down text into smaller units, like words or sentences.\n",
        "* Tagging: Labeling words with their part of speech.\n",
        "* Stemming and Lemmatization: Reducing words to their base or root form.\n",
        "* Parsing: Analyzing the grammatical structure of sentences.\n",
        "* Semantic Reasoning: Understanding the meaning of text."
      ],
      "metadata": {
        "id": "xGKWYB-yAzti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy"
      ],
      "metadata": {
        "id": "4sVeN4SVBuZ6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "spacy.cli.download(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "EJvF6VYQB3h4",
        "outputId": "d13c3aca-2486-4070-aa87-60fe61da1ef1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "sp9p7vodChQU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "6StsPnsyCrOL",
        "outputId": "5d11fa04-47aa-4746-a93f-78c0ff6cfcf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ain', 'don', 'was', 'this', 'then', \"we'll\", \"doesn't\", \"they've\", 'before', 'ma', 'you', 'won', 'how', 'needn', 'the', \"hadn't\", 'aren', \"it'll\", 'than', 'o', 'while', 'once', 'having', \"it's\", \"they'd\", 'now', 'with', 'nor', 'out', 'were', \"you're\", \"it'd\", \"shan't\", 'more', 're', 'by', \"she'd\", \"she'll\", \"we're\", 'do', 'does', \"he's\", 'me', 'ours', 'that', 'all', 'they', 'her', 'didn', 'which', 'itself', 'both', 'should', 'did', \"we've\", 'm', 'further', 'wasn', 'of', \"i'm\", \"couldn't\", 'had', 'if', \"he'll\", 'him', 'ourselves', \"haven't\", 's', 'their', 'under', 'haven', 'hers', 'can', 'down', 'up', \"you'd\", 'mustn', 'each', 'why', 'couldn', 'it', 'on', 'between', 'some', 'off', \"they'll\", 'be', 've', 'hadn', 'we', \"weren't\", 'not', 'over', 'any', 'few', 'shouldn', 'weren', 'will', 'd', 'into', 'wouldn', 'being', 'when', \"wouldn't\", 'these', 'against', 'here', 'our', 'shan', 'just', 'in', 'he', 'themselves', 'yourself', \"you've\", \"shouldn't\", 'above', 'yours', \"won't\", 'his', 'himself', 'its', \"isn't\", 'herself', 'same', 'only', 'is', 'hasn', 'mightn', 'below', \"i'll\", 'to', 'during', 't', 'again', 'and', 'has', \"aren't\", \"we'd\", \"you'll\", 'very', \"needn't\", 'for', 'a', 'am', 'or', 'there', 'but', \"i've\", \"hasn't\", \"should've\", 'who', 'isn', 'your', 'theirs', 'from', 'have', 'them', 'those', 'yourselves', 'so', 'whom', 'other', \"didn't\", 'y', \"mustn't\", 'most', 'myself', 'my', 'where', 'she', 'after', \"mightn't\", 'i', 'been', 'doing', \"she's\", 'at', 'an', \"don't\", 'own', \"he'd\", 'll', 'about', 'what', 'are', \"i'd\", \"they're\", 'until', 'as', 'such', 'through', 'too', 'because', \"that'll\", \"wasn't\", 'no', 'doesn'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Natural Language Processing (NLP) is a field that ' \\\n",
        "'combines computer science, artificial intelligence and ' \\\n",
        "'language studies. It helps computers understand, process and ' \\\n",
        "'create human language in a way that makes sense and is useful. ' \\\n",
        "'With the growing amount of text data from social media, ' \\\n",
        "'websites and other sources, NLP is becoming a key tool to gain ' \\\n",
        "'insights and automate tasks like analyzing text or translating ' \\\n",
        "'languages.'"
      ],
      "metadata": {
        "id": "SCbOoS3-Cmdz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Conver into lowercharacter"
      ],
      "metadata": {
        "id": "S_EIeFywDDQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "qvkDDOZ3DC0u"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(token)"
      ],
      "metadata": {
        "id": "zSlLe0kmDPPD",
        "outputId": "9cc7f51b-07ef-48ab-998c-06ec51e8dfd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'that', 'combines', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'language', 'studies', '.', 'It', 'helps', 'computers', 'understand', ',', 'process', 'and', 'create', 'human', 'language', 'in', 'a', 'way', 'that', 'makes', 'sense', 'and', 'is', 'useful', '.', 'With', 'the', 'growing', 'amount', 'of', 'text', 'data', 'from', 'social', 'media', ',', 'websites', 'and', 'other', 'sources', ',', 'NLP', 'is', 'becoming', 'a', 'key', 'tool', 'to', 'gain', 'insights', 'and', 'automate', 'tasks', 'like', 'analyzing', 'text', 'or', 'translating', 'languages', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lower_tokens = [t.lower() for t in token]\n",
        "lower_tokens"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MYc8zaciF8d2",
        "outputId": "965f87ad-f077-4949-a6f0-2d8d1e7a0a1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '(',\n",
              " 'nlp',\n",
              " ')',\n",
              " 'is',\n",
              " 'a',\n",
              " 'field',\n",
              " 'that',\n",
              " 'combines',\n",
              " 'computer',\n",
              " 'science',\n",
              " ',',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'and',\n",
              " 'language',\n",
              " 'studies',\n",
              " '.',\n",
              " 'it',\n",
              " 'helps',\n",
              " 'computers',\n",
              " 'understand',\n",
              " ',',\n",
              " 'process',\n",
              " 'and',\n",
              " 'create',\n",
              " 'human',\n",
              " 'language',\n",
              " 'in',\n",
              " 'a',\n",
              " 'way',\n",
              " 'that',\n",
              " 'makes',\n",
              " 'sense',\n",
              " 'and',\n",
              " 'is',\n",
              " 'useful',\n",
              " '.',\n",
              " 'with',\n",
              " 'the',\n",
              " 'growing',\n",
              " 'amount',\n",
              " 'of',\n",
              " 'text',\n",
              " 'data',\n",
              " 'from',\n",
              " 'social',\n",
              " 'media',\n",
              " ',',\n",
              " 'websites',\n",
              " 'and',\n",
              " 'other',\n",
              " 'sources',\n",
              " ',',\n",
              " 'nlp',\n",
              " 'is',\n",
              " 'becoming',\n",
              " 'a',\n",
              " 'key',\n",
              " 'tool',\n",
              " 'to',\n",
              " 'gain',\n",
              " 'insights',\n",
              " 'and',\n",
              " 'automate',\n",
              " 'tasks',\n",
              " 'like',\n",
              " 'analyzing',\n",
              " 'text',\n",
              " 'or',\n",
              " 'translating',\n",
              " 'languages',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Filter out those are not parts of stopwords\n",
        "\n",
        "Stop words in NLP are common, high-frequency words like \"a,\" \"the,\" and \"is\" that are often removed from text because they add little meaning to the core content.\n",
        "\n",
        "Removing them reduces noise and computational load, helping NLP tasks like search, summarization, and machine learning focus on more significant words."
      ],
      "metadata": {
        "id": "FnrzEbevDXYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens = [t for t in lower_tokens if t.isalpha() and t not in stop_words]\n"
      ],
      "metadata": {
        "id": "8tw5XYtaDUk5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered_tokens)"
      ],
      "metadata": {
        "id": "p_FTNsF2Estq",
        "outputId": "19a5ddc4-f0bc-4071-ee92-5c6fcf17ae87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'nlp', 'field', 'combines', 'computer', 'science', 'artificial', 'intelligence', 'language', 'studies', 'helps', 'computers', 'understand', 'process', 'create', 'human', 'language', 'way', 'makes', 'sense', 'useful', 'growing', 'amount', 'text', 'data', 'social', 'media', 'websites', 'sources', 'nlp', 'becoming', 'key', 'tool', 'gain', 'insights', 'automate', 'tasks', 'like', 'analyzing', 'text', 'translating', 'languages']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Stemmer and lemmatizer\n",
        "\n",
        "Stemming and lemmatization are two text normalization techniques in NLP that reduce words to a **base or root form**.\n",
        "\n",
        "**Stemming** uses a simple heuristic to cut off word endings, which can result in a non-word.\n",
        "\n",
        "**Lemmatization** uses a dictionary and analyzes the word's context and part of speech to find a grammatically correct base form (lemma). Lemmatization is more accurate but slower, while stemming is faster but less accurate"
      ],
      "metadata": {
        "id": "oj-LDiYnFANx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "steamed_word = [ stemmer.stem(t) for t in filtered_tokens ]\n",
        "lemmatized_word = [ lemmatizer.lemmatize(t) for t in filtered_tokens]"
      ],
      "metadata": {
        "id": "n-J0xIbTEqDH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(steamed_word)"
      ],
      "metadata": {
        "id": "97o8HeiOFbdF",
        "outputId": "eb0d99e8-bf2c-44b4-d2a4-acd8035d0584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natur', 'languag', 'process', 'nlp', 'field', 'combin', 'comput', 'scienc', 'artifici', 'intellig', 'languag', 'studi', 'help', 'comput', 'understand', 'process', 'creat', 'human', 'languag', 'way', 'make', 'sens', 'use', 'grow', 'amount', 'text', 'data', 'social', 'media', 'websit', 'sourc', 'nlp', 'becom', 'key', 'tool', 'gain', 'insight', 'autom', 'task', 'like', 'analyz', 'text', 'translat', 'languag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatized_word)"
      ],
      "metadata": {
        "id": "tU1pv299GHdC",
        "outputId": "ecafb48b-04de-4dad-8c62-d301a1494072",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'nlp', 'field', 'combine', 'computer', 'science', 'artificial', 'intelligence', 'language', 'study', 'help', 'computer', 'understand', 'process', 'create', 'human', 'language', 'way', 'make', 'sense', 'useful', 'growing', 'amount', 'text', 'data', 'social', 'medium', 'website', 'source', 'nlp', 'becoming', 'key', 'tool', 'gain', 'insight', 'automate', 'task', 'like', 'analyzing', 'text', 'translating', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(sentence)\n",
        "\n",
        "for sentence in doc.sents:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "oPHhYPQuEpv3",
        "outputId": "f3398531-eb3d-4225-acaf-74778557acf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing (NLP) is a field that combines computer science, artificial intelligence and language studies.\n",
            "It helps computers understand, process and create human language in a way that makes sense and is useful.\n",
            "With the growing amount of text data from social media, websites and other sources, NLP is becoming a key tool to gain insights and automate tasks like analyzing text or translating languages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk0ij7kRCTj2",
        "outputId": "4f67febf-3e14-4c7a-e287-f759c6b78159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [['natural', 'language', 'processing', 'nlp', 'field', 'combine', 'computer', 'science', 'artificial', 'intelligence', 'language', 'study', 'help', 'computer', 'understand', 'process', 'create', 'human', 'language', 'way', 'make', 'sense', 'useful', 'growing', 'amount', 'text', 'data', 'social', 'medium', 'website', 'source', 'nlp', 'becoming', 'key', 'tool', 'gain', 'insight', 'automate', 'task', 'like', 'analyzing', 'text', 'translating', 'language']]\n",
        "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, workers=4)\n",
        "print(model.wv['natural'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsFAMIG8C_C1",
        "outputId": "08fb9251-7403-4cdc-8d4a-c520b9e2ad3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.01037544 -0.01479442 -0.00582388 -0.00172863  0.00705572  0.01948378\n",
            " -0.00677855  0.00380354  0.01936202  0.00306317  0.00197299  0.01960474\n",
            "  0.01859092  0.01541613 -0.01234105  0.01996797  0.01169797  0.01814533\n",
            " -0.0039904   0.00669987  0.01366711 -0.00778751  0.0132857   0.00512573\n",
            "  0.01862746 -0.00607161 -0.00621874  0.01243078 -0.01815649 -0.01450798\n",
            " -0.01300005 -0.00149815 -0.00472604  0.01363104  0.01847318 -0.00181952\n",
            "  0.00282565  0.00404071 -0.0040396  -0.01606868  0.0148821  -0.0085958\n",
            "  0.00915304  0.01817941  0.00608644  0.00627758  0.00812366 -0.00540243\n",
            "  0.00764954  0.00067525]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in sentences:\n",
        "  print(model.wv[token])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzK7wQdpDgm8",
        "outputId": "7293cf7b-0cac-4b70-df25-efe5eb830e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.01037544 -0.01479442 -0.00582388 ... -0.00540243  0.00764954\n",
            "   0.00067525]\n",
            " [-0.00108949  0.00043467  0.01020348 ...  0.019192    0.00998422\n",
            "   0.01846741]\n",
            " [ 0.01946244 -0.01958262 -0.01299524 ...  0.00045938  0.01892731\n",
            "  -0.00520865]\n",
            " ...\n",
            " [-0.01632091  0.00896744 -0.00827308 ... -0.01409276  0.00181619\n",
            "   0.012786  ]\n",
            " [ 0.00018957  0.0061436  -0.01361971 ... -0.00540813 -0.00872874\n",
            "  -0.00206447]\n",
            " [-0.00108949  0.00043467  0.01020348 ...  0.019192    0.00998422\n",
            "   0.01846741]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lNPQc_0nECMH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}