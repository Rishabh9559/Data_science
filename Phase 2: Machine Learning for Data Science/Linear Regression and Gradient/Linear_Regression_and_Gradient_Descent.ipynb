{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4eSlAOC8KtYC67PKaresU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishabh9559/Data_science/blob/main/Phase%202%3A%20Machine%20Learning%20for%20Data%20Science/Linear%20Regression%20and%20Gradient/Linear_Regression_and_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear_Regression_and_Gradient_Descent**"
      ],
      "metadata": {
        "id": "DWGqueSstz68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##  **1. What is Linear Regression?**\n",
        "\n",
        "**Linear Regression** is a **supervised learning** algorithm used for **predicting a continuous value**.\n",
        "\n",
        "It models the relationship between:\n",
        "\n",
        "* üìà **Independent variable(s)** (input `x`)\n",
        "* üìâ **Dependent variable** (output `y`)\n",
        "\n",
        "### üîß Equation of a Line:\n",
        "\n",
        "$$\n",
        "y = mx + c \\quad \\text{or} \\quad y = w_1x + w_0\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* `y` = predicted value\n",
        "* `x` = input feature\n",
        "* `w‚ÇÅ` = weight (slope)\n",
        "* `w‚ÇÄ` = bias/intercept\n",
        "\n",
        "---\n",
        "\n",
        "##  **2. Goal of Linear Regression:**\n",
        "\n",
        "Find the **best fit line** (values of `w‚ÇÅ` and `w‚ÇÄ`) that **minimizes the error** between predicted and actual values.\n",
        "\n",
        "---\n",
        "\n",
        "##  **3. What is Gradient Descent?**\n",
        "\n",
        "**Gradient Descent** is an **optimization algorithm** used to minimize a **loss function** (like MSE - Mean Squared Error).\n",
        "\n",
        "###  Loss Function (MSE):\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $y_i$ = actual value\n",
        "* $\\hat{y}_i$ = predicted value\n",
        "* `n` = number of data points\n",
        "\n",
        "---\n",
        "\n",
        "##  **4. How Gradient Descent Works:**\n",
        "\n",
        "1. Start with random weights (`w‚ÇÄ`, `w‚ÇÅ`)\n",
        "2. Calculate **error/loss** using MSE\n",
        "3. Compute **gradients** (derivatives of loss w\\.r.t weights)\n",
        "4. **Update weights**:\n",
        "\n",
        "   $$\n",
        "   w := w - \\alpha \\cdot \\frac{\\partial \\text{Loss}}{\\partial w}\n",
        "   $$\n",
        "\n",
        "   where `Œ±` is the **learning rate**\n",
        "5. Repeat until convergence (loss is minimal)\n",
        "\n",
        "---\n",
        "\n",
        "##  Example (1D Linear Regression):\n",
        "\n",
        "Given data:\n",
        "\n",
        "```\n",
        "x: [1, 2, 3, 4]\n",
        "y: [2, 4, 6, 8]\n",
        "```\n",
        "\n",
        "Initialize `w‚ÇÄ=0`, `w‚ÇÅ=0`\n",
        "\n",
        "* Predict $\\hat{y}_i = w_1x_i + w_0$\n",
        "* Use gradient descent to update `w‚ÇÅ` and `w‚ÇÄ` to minimize error\n",
        "\n",
        "Eventually, it will learn:\n",
        "\n",
        "$$\n",
        "\\hat{y} = 2x \\quad (\\text{ideal line})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lG9Rty3zt5zD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error\n",
        "\n",
        "## **MSE - Mean Square Erro**\n",
        "\n",
        "\n",
        "MSE is a common **loss function** used in **regression problems** to measure the **average of the squared differences** between actual and predicted values.\n",
        "\n",
        "---\n",
        "\n",
        "###  **MSE Formula:**\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $y_i$ = Actual value\n",
        "* $\\hat{y}_i$ = Predicted value\n",
        "* $n$ = Total number of data points\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Square the Error?**\n",
        "\n",
        "* Avoids negative errors cancelling out positive ones.\n",
        "* Penalizes large errors more heavily than small ones.\n",
        "\n",
        "---\n",
        "\n",
        "###  Example:\n",
        "\n",
        "Let‚Äôs say:\n",
        "\n",
        "* Actual values $y = [3, 5, 7]$\n",
        "* Predicted values $\\hat{y} = [2, 5, 10]$\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{3}[(3-2)^2 + (5-5)^2 + (7-10)^2] \\\\\n",
        "= \\frac{1}{3}[1 + 0 + 9] = \\frac{10}{3} \\approx 3.33\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rRvPvEBhvGwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u9hQz8rXwZsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üìè 1. **Best Fit Line** (in Linear Regression)\n",
        "\n",
        "The **best fit line** is the line that **best represents the data** in a **scatter plot** by minimizing the **distance between the actual points and the predicted line**.\n",
        "\n",
        "### üîß Equation of the best fit line:\n",
        "\n",
        "$$\n",
        "y = w_1x + w_0\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $x$: input (independent variable)\n",
        "* $y$: output (dependent variable)\n",
        "* $w_1$: slope\n",
        "* $w_0$: intercept\n",
        "\n",
        "### üéØ Goal:\n",
        "\n",
        "Minimize the **total error** between predicted values ($\\hat{y}$) and actual values ($y$).\n",
        "\n",
        "We use algorithms like **Gradient Descent** to adjust `w‚ÇÅ` and `w‚ÇÄ` so that this line fits the data as accurately as possible.\n",
        "\n",
        "---\n",
        "\n",
        "##  2. **Residual Error**\n",
        "\n",
        "\n",
        "\n",
        "### üîç What is Residual Error?\n",
        "\n",
        "Residual Error is the **difference between the actual value** and the **predicted value** from the model.\n",
        "\n",
        "###  Formula:\n",
        "\n",
        "$$\n",
        "\\text{Residual} = y_i - \\hat{y}_i\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $y_i$: actual value\n",
        "* $\\hat{y}_i$: predicted value from the best fit line\n",
        "\n",
        "Each data point has its **own residual**.\n",
        "\n",
        "---\n",
        "\n",
        "### üîé Visual Intuition:\n",
        "\n",
        "* If a point lies **above** the line ‚Üí residual is **positive**\n",
        "* If it lies **below** the line ‚Üí residual is **negative**\n",
        "* If it lies **on** the line ‚Üí residual is **zero**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2A0ttzjJwiet"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35IRwm6stLa8"
      },
      "outputs": [],
      "source": []
    }
  ]
}