{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPEKpSZTT+SlSkQiXgGOdb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishabh9559/Data_science/blob/main/Phase%202%3A%20Machine%20Learning%20for%20Data%20Science/Dimensionally_reduction_techniques/Dimensionally_reduction_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionally_reduction_techniques"
      ],
      "metadata": {
        "id": "L_qYyB6__saQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimension -> feature, Columns"
      ],
      "metadata": {
        "id": "KflX2AWlBitP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **What is Dimensionality Reduction?**\n",
        "\n",
        "In Machine Learning and Data Science, **dimensionality reduction** means reducing the number of input variables (features) in a dataset while keeping as much useful information as possible.\n",
        "\n",
        "* High-dimensional data (many features) → harder to analyze, visualize, and train models on.\n",
        "* Dimensionality reduction helps simplify the dataset, improve performance, and reduce noise.\n",
        "\n",
        "\n",
        "\n",
        "### **Why is it needed?**\n",
        "\n",
        "* To remove irrelevant/noisy features.\n",
        "* To **avoid the \"curse of dimensionality\"** (performance issues when data has too many features).\n",
        "* To **visualize data** in 2D or 3D.\n",
        "* To **reduce computation cost** for ML algorithms.\n",
        "\n",
        "\n",
        "\n",
        "### **Types of Dimensionality Reduction Techniques**\n",
        "\n",
        "1. **Feature Selection (choose important features)**\n",
        "\n",
        "   * Instead of reducing dimensions mathematically, we just **pick the best subset of features**.\n",
        "   * Examples:\n",
        "\n",
        "     * Filter methods (Correlation, Chi-square test, ANOVA)\n",
        "     * Wrapper methods (Forward selection, Backward elimination)\n",
        "     * Embedded methods (LASSO, Decision Trees feature importance)\n",
        "\n",
        "\n",
        "\n",
        "2. **Feature Extraction (transform data into new dimensions)**\n",
        "\n",
        "   * We create **new features** by combining or transforming original features.\n",
        "   * Techniques:\n",
        "\n",
        "   **a) Principal Component Analysis (PCA)**\n",
        "\n",
        "   * Converts correlated features into fewer **uncorrelated principal components**.\n",
        "   * Keeps maximum variance (information).\n",
        "   * Widely used for compression & visualization.\n",
        "\n",
        "   **b) Linear Discriminant Analysis (LDA)**\n",
        "\n",
        "   * Supervised technique.\n",
        "   * Reduces dimensions by maximizing class separability.\n",
        "   * Used in classification tasks.\n",
        "\n",
        "   **c) t-SNE (t-Distributed Stochastic Neighbor Embedding)**\n",
        "\n",
        "   * Non-linear technique for **visualizing high-dimensional data in 2D/3D**.\n",
        "   * Preserves local structure (good for clustering visualization).\n",
        "\n",
        "   **d) Autoencoders (Deep Learning)**\n",
        "\n",
        "   * Neural networks that learn compressed (lower-dimensional) representations.\n",
        "   * Useful for very high-dimensional data like images.\n",
        "\n",
        "   **e) UMAP (Uniform Manifold Approximation and Projection)**\n",
        "\n",
        "   * Like t-SNE, but faster and preserves global + local structures better.\n",
        "\n",
        "\n",
        "\n",
        "### **Quick Example**\n",
        "\n",
        "Imagine you have a dataset of **100 features** describing patients’ health.\n",
        "\n",
        "* Many features are correlated (e.g., weight, BMI, waist size).\n",
        "* Instead of training an ML model on all 100 features, we can apply PCA → reduce it to maybe **10 principal components** that still capture **90–95% of the information**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p9b7tsDfBhjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Principal Component Analysis (PCA)**\n",
        "**In short:**\n",
        "PCA = A way to reduce many correlated features into fewer uncorrelated features (principal components), keeping as much variance (information) as possible.\n",
        "\n",
        "### **What is PCA?**\n",
        "\n",
        "* PCA is a **dimensionality reduction technique**.\n",
        "* It transforms high-dimensional data into a **smaller set of uncorrelated variables** called **principal components**.\n",
        "* These components capture the **maximum variance (information)** from the original data.\n",
        "* PCA is an Unsupervised learning technique.\n",
        "\n",
        "\n",
        "\n",
        "### **Key Idea**\n",
        "\n",
        "* In a dataset, many features are often **correlated**.\n",
        "* PCA finds new axes (directions) in such a way that:\n",
        "\n",
        "  1. The **first principal component (PC1)** captures the maximum variance.\n",
        "  2. The **second principal component (PC2)** is orthogonal (uncorrelated) to PC1 and captures the next highest variance.\n",
        "  3. This continues until all variance is explained.\n",
        "\n",
        "So instead of working with 100 correlated features, PCA may reduce it to **10 uncorrelated principal components**.\n",
        "\n",
        "\n",
        "\n",
        "### **Steps in PCA**\n",
        "\n",
        "1. **Standardize the data** (make features have mean = 0, variance = 1).\n",
        "2. **Compute covariance matrix** (relationship between features).\n",
        "3. **Find eigenvalues & eigenvectors** of covariance matrix.\n",
        "\n",
        "   * Eigenvectors = directions of principal components.\n",
        "   * Eigenvalues = amount of variance captured.\n",
        "4. **Sort eigenvalues** (biggest first) → select top *k* components.\n",
        "5. **Project data** onto the new *k* dimensions.\n",
        "\n",
        "\n",
        "\n",
        "### **Example**\n",
        "\n",
        "Suppose you have data with two features:\n",
        "\n",
        "* Height (cm)\n",
        "* Weight (kg)\n",
        "\n",
        "They are correlated (taller people usually weigh more).\n",
        "\n",
        "* PCA finds a **new axis (PC1)** along the direction of maximum variance (Height+Weight combination).\n",
        "* Then, it may reduce to **1D data** instead of 2D.\n",
        "\n",
        "\n",
        "\n",
        "### **Applications of PCA**\n",
        "\n",
        "* **Data compression** (reduce storage and computation).\n",
        "* **Noise reduction** (remove less informative dimensions).\n",
        "* **Visualization** (reduce data to 2D or 3D for plotting).\n",
        "* **Preprocessing** before machine learning to improve performance.\n",
        "\n",
        "\n",
        "\n",
        "### **Advantages**\n",
        "\n",
        "* Reduces dimensionality without much loss of information.\n",
        "* Removes correlation (components are independent).\n",
        "* Makes models faster & less prone to overfitting.\n",
        "\n",
        "### **Limitations**\n",
        "\n",
        "* PCA is **linear** (may not capture complex patterns).\n",
        "* Harder to interpret principal components (they are combinations, not original features).\n",
        "* Sensitive to scaling (always standardize data before PCA).\n"
      ],
      "metadata": {
        "id": "93wUifS0DIBr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4j6jRi2H_dQc"
      },
      "outputs": [],
      "source": []
    }
  ]
}