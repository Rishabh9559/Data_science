{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVSOwsv4YkQQxps7S6aGg4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishabh9559/Data_science/blob/main/Phase%202%3A%20Machine%20Learning%20for%20Data%20Science/Dimensionally_reduction_techniques/Dimensionally_reduction_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionally_reduction_techniques"
      ],
      "metadata": {
        "id": "L_qYyB6__saQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimension -> feature, Columns"
      ],
      "metadata": {
        "id": "KflX2AWlBitP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **What is Dimensionality Reduction?**\n",
        "\n",
        "In Machine Learning and Data Science, **dimensionality reduction** means reducing the number of input variables (features) in a dataset while keeping as much useful information as possible.\n",
        "\n",
        "* High-dimensional data (many features) → harder to analyze, visualize, and train models on.\n",
        "* Dimensionality reduction helps simplify the dataset, improve performance, and reduce noise.\n",
        "\n",
        "\n",
        "\n",
        "### **Why is it needed?**\n",
        "\n",
        "* To remove irrelevant/noisy features.\n",
        "* To **avoid the \"curse of dimensionality\"** (performance issues when data has too many features).\n",
        "* To **visualize data** in 2D or 3D.\n",
        "* To **reduce computation cost** for ML algorithms.\n",
        "\n",
        "\n",
        "\n",
        "### **Types of Dimensionality Reduction Techniques**\n",
        "\n",
        "1. **Feature Selection (choose important features)**\n",
        "\n",
        "   * Instead of reducing dimensions mathematically, we just **pick the best subset of features**.\n",
        "   * Examples:\n",
        "\n",
        "     * Filter methods (Correlation, Chi-square test, ANOVA)\n",
        "     * Wrapper methods (Forward selection, Backward elimination)\n",
        "     * Embedded methods (LASSO, Decision Trees feature importance)\n",
        "\n",
        "\n",
        "\n",
        "2. **Feature Extraction (transform data into new dimensions)**\n",
        "\n",
        "   * We create **new features** by combining or transforming original features.\n",
        "   * Techniques:\n",
        "\n",
        "   **a) Principal Component Analysis (PCA)**\n",
        "\n",
        "   * Converts correlated features into fewer **uncorrelated principal components**.\n",
        "   * Keeps maximum variance (information).\n",
        "   * Widely used for compression & visualization.\n",
        "\n",
        "   **b) Linear Discriminant Analysis (LDA)**\n",
        "\n",
        "   * Supervised technique.\n",
        "   * Reduces dimensions by maximizing class separability.\n",
        "   * Used in classification tasks.\n",
        "\n",
        "   **c) t-SNE (t-Distributed Stochastic Neighbor Embedding)**\n",
        "\n",
        "   * Non-linear technique for **visualizing high-dimensional data in 2D/3D**.\n",
        "   * Preserves local structure (good for clustering visualization).\n",
        "\n",
        "   **d) Autoencoders (Deep Learning)**\n",
        "\n",
        "   * Neural networks that learn compressed (lower-dimensional) representations.\n",
        "   * Useful for very high-dimensional data like images.\n",
        "\n",
        "   **e) UMAP (Uniform Manifold Approximation and Projection)**\n",
        "\n",
        "   * Like t-SNE, but faster and preserves global + local structures better.\n",
        "\n",
        "\n",
        "\n",
        "### **Quick Example**\n",
        "\n",
        "Imagine you have a dataset of **100 features** describing patients’ health.\n",
        "\n",
        "* Many features are correlated (e.g., weight, BMI, waist size).\n",
        "* Instead of training an ML model on all 100 features, we can apply PCA → reduce it to maybe **10 principal components** that still capture **90–95% of the information**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p9b7tsDfBhjB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4j6jRi2H_dQc"
      },
      "outputs": [],
      "source": []
    }
  ]
}